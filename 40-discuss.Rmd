# Discussion {#discus}

Our final logistic regression model has Age.Cat, Conc.decrease, Gender, Genetics, CFA, VehicleCrashes, Humor, GE.Reflux, Diabetes, Driver, Hypothyroidism, Stroke, ESS.Cat, Wit.Apneas as predictor variables.  

Although for our first model (table \@ref(tab:both)) we obtained and accuracy of *`r round(sum(diag(acc.both))/sum(acc.both), 2)`*, we tried to define a new model (table \@ref(tab:new)) more simple (with less variables) even if we obtained a lower accuracy (*`r round(sum(diag(acc.new))/sum(acc.new), 2)`*). 

Ideally, in a logistic regression model, the proportion of events (with) and non-events (without) in the Y variable (OSA) should approximately be the same. In this analysis we have 2/3 of patients with OSA and only 1/3 without OSA. So we could sample the observations in approximately equal proportions to get better models as a sensitivity analysis, but we didn't do it.

In this case we use p-values to select the variables to use in our model but if we had used a very large dataset we could instead find the information value of variables to get an idea of how valuable they are in explaining the dependent variable (OSA).
We couldn't replicate this traditional approach in the selection of variables to define a predictive logistic regression model if we worked with “too large” datasets because we would have statistical significant results for all the variables in the univariate analysis.


<img src="images/logoFMUP.png" alt="logotipo FMUP" style="width:100px;height:40px;" align="right">

