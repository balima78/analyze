[
["index.html", "Basics of Health Intelligent Data Analysis", " Basics of Health Intelligent Data Analysis Bruno Lima, Cátia Redondo 2020-01-21 "],
["preface.html", "Preface", " Preface This is a book written in Markdown through RStudio. The bookdown package (Xie 2015) can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) In this book, we will present our results for the work we made on the subject of Basics of Health Intelligent Data Analysis from the HEADS PhD programme. An exhaustive explanation using the bookdown package (Xie 2019) can be found at bookdown: Authoring Books and Technical Documents with R Markdown and this is only a sample book, which was built on top of R Markdown and knitr This book is publish through NETLIFY as described by C.M. "],
["intro.html", "1 Introduction", " 1 Introduction Obstructive sleep apnoea (OSA) is a potentially serious sleep disorder. This is a common disorder characterized by repetitive episodes of nocturnal breathing cessation due to upper airway collapse. The obstructive events (apneas or hypopneas) cause a progressive asphyxia that increasingly stimulates breathing efforts against the collapsed airway. The aetiology of OSA is multifactorial, consisting of a complex interplay between anatomic, neuromuscular factors and an underlying genetic predisposition toward the disease. Risk factors include: snoring, male gender, middle age, menopause in women, obesity, narrowed airway, hypertension, chronic nasal congestion, smoking, diabetes, family history of sleep apnea, asthma, large neck circumference, enlarged tonsils/adenoids and low-lying soft palate. Complications from recurrent OSA may include: daytime fatigue and sleepiness, cardiovascular problems, complications with medications and surgery, eye problems, or sleep-deprived partners. Also, people with obstructive sleep apnea may also complain of memory problems, morning headaches, mood swings or feelings of depression, and a need to urinate frequently at night (nocturia). In this exercise, our primary aim is to identify predictive factors for OSA, using a multivariable logistic regression model. "],
["method.html", "2 Methods", " 2 Methods Database used in this study includes simulated data from 523 patients who were referred to sleep consultation for risk of OSA. As two patients have no data for OSA, we considered on our analysis 521 patients. A total of 38 variables (table 2.1) were recorded as potential predictors for OSA. Patient characteristics were described using contingency tables. Univariate tests comparing categorical variables were conducted using univariate logistic regression. Variables with p-values &lt; 0.2 were selected for a stepwise logistic regression direction both that consists of iteratively adding and removing predictors. From this model we tried to define a new model with lower number of prediction variables. For measuring the performance of the predictive model we compute it’s accuracy value and ROC curve. We did not split our data in training and a test sets, so the performance of the model was assessed in the same data used for modelling. At the final (‘new model’) we also estimated the model performance with Leave-one-out cross-validation. To assess multicollinearity of the variables in the model we computed the variance inflation factors (VIF). As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity. Table 2.1: potential prediction variables variable type AC.Cat categorical AF categorical Age.Cat categorical Alcohol categorical BMI.Cat categorical CFA categorical CHF categorical Chocking categorical Coffee categorical Conc.decrease categorical Daytime.Sleepiness categorical Decreased.Libido categorical Depression categorical Diabetes categorical Driver categorical Dyslipidemia categorical ESS.Cat categorical GE.Reflux categorical Gender categorical Genetics categorical Headaches categorical Humor categorical Hypertension categorical Hypothyroidism categorical MI categorical NC.Cat categorical Nocturia categorical PHT categorical Race categorical RefreshingSleep categorical Renal.Failure categorical RestlessSleep categorical Sedatives categorical Smoking categorical Snore categorical Stroke categorical VehicleCrashes categorical Wit.Apneas categorical All the analysis and graphic representations where performed in RStudio, an integrated development environment (IDE) for R programming language. "],
["result.html", "3 Results", " 3 Results In this study we analysed a total of 521 subjects, from those 325 (62.38%) are male, and 355 (68.14%) have OSA. When we analysed missing data (Figure 3.1) we do not found any variable with an excessive number of missing values. Figure 3.1: Missing values for each variable 1/2 Figure 3.2: Missing values for each variable 2/2 "],
["results1.html", "3.1 Descriptive analysis", " 3.1 Descriptive analysis We begin designing contingency tables for each variable with OSA (Table 3.1). In these tables ‘Yes’ stand for the number of patients with OSA, ‘yes.%’ stands for percentage of patients with OSA by column, ‘No’ stands for number of patients without OSA, and ‘no.%’ stands for the percentage of patients without OSA by column. Table 3.1: AC.Cat AC.Cat Yes yes.% No no.% Normal 29 8.22 14 8.48 Increased 324 91.78 151 91.52 Table 3.1: AF AF Yes yes.% No no.% No 261 75 158 95.18 Yes 87 25 8 4.82 Table 3.1: Age.Cat Age.Cat Yes yes.% No no.% &lt;40 14 3.99 37 22.29 40-54 73 20.80 73 43.98 55-69 169 48.15 44 26.51 &gt;=70 95 27.07 12 7.23 Table 3.1: Alcohol Alcohol Yes yes.% No no.% No 116 33.05 81 49.09 Yes 235 66.95 84 50.91 Table 3.1: BMI.Cat BMI.Cat Yes yes.% No no.% Normal 179 50.85 91 55.49 Obese 173 49.15 73 44.51 Table 3.1: CFA CFA Yes yes.% No no.% No 13 3.72 57 35.19 Yes 336 96.28 105 64.81 Table 3.1: CHF CHF Yes yes.% No no.% No 258 73.3 141 87.04 Yes 94 26.7 21 12.96 Table 3.1: Chocking Chocking Yes yes.% No no.% No 143 40.62 60 36.59 Yes 209 59.38 104 63.41 Table 3.1: Coffee Coffee Yes yes.% No no.% No 41 11.75 21 12.73 Yes 308 88.25 144 87.27 Table 3.1: Conc.decrease Conc.decrease Yes yes.% No no.% No 170 48.57 49 29.7 Yes 180 51.43 116 70.3 Table 3.1: Daytime.Sleepiness Daytime.Sleepiness Yes yes.% No no.% No 85 24.22 5 3.01 Yes 266 75.78 161 96.99 Table 3.1: Decreased.Libido Decreased.Libido Yes yes.% No no.% No 2 0.57 62 37.8 Yes 350 99.43 102 62.2 Table 3.1: Depression Depression Yes yes.% No no.% No 23 6.52 9 5.45 Yes 330 93.48 156 94.55 Table 3.1: Diabetes Diabetes Yes yes.% No no.% No 156 43.94 90 54.22 Yes 199 56.06 76 45.78 Table 3.1: Driver Driver Yes yes.% No no.% No 313 88.42 155 93.94 Yes 41 11.58 10 6.06 Table 3.1: Dyslipidemia Dyslipidemia Yes yes.% No no.% No 21 5.97 47 28.48 Yes 331 94.03 118 71.52 Table 3.1: ESS.Cat ESS.Cat Yes yes.% No no.% Normal 160 45.71 39 23.78 Daytime Sleepiness 190 54.29 125 76.22 Table 3.1: GE.Reflux GE.Reflux Yes yes.% No no.% No 237 67.71 150 92.02 Yes 113 32.29 13 7.98 Table 3.1: Gender Gender Yes yes.% No no.% Female 99 28.12 91 55.83 Male 253 71.88 72 44.17 Table 3.1: Genetics Genetics Yes yes.% No no.% No 265 75.28 62 37.8 Yes 87 24.72 102 62.2 Table 3.1: Headaches Headaches Yes yes.% No no.% No 143 40.4 75 45.18 Yes 211 59.6 91 54.82 Table 3.1: Humor Humor Yes yes.% No no.% No 8 2.27 27 16.27 Yes 344 97.73 139 83.73 Table 3.1: Hypertension Hypertension Yes yes.% No no.% No 50 14.29 58 35.37 Yes 300 85.71 106 64.63 Table 3.1: Hypothyroidism Hypothyroidism Yes yes.% No no.% No 197 56.45 122 74.39 Yes 152 43.55 42 25.61 Table 3.1: MI MI Yes yes.% No no.% No 270 76.49 142 86.06 Yes 83 23.51 23 13.94 Table 3.1: NC.Cat NC.Cat Yes yes.% No no.% Normal 137 38.92 88 53.66 Increased 215 61.08 76 46.34 Table 3.1: Nocturia Nocturia Yes yes.% No no.% No 116 33.05 79 47.59 Yes 235 66.95 87 52.41 Table 3.1: PHT PHT Yes yes.% No no.% No 89 25.21 107 64.85 Yes 264 74.79 58 35.15 Table 3.1: Race Race Yes yes.% No no.% African 7 2.01 152 92.68 Caucasian 342 97.99 12 7.32 Table 3.1: RefreshingSleep RefreshingSleep Yes yes.% No no.% No 279 78.81 130 80.75 Yes 75 21.19 31 19.25 Table 3.1: Renal.Failure Renal.Failure Yes yes.% No no.% No 221 62.78 131 79.88 Yes 131 37.22 33 20.12 Table 3.1: RestlessSleep RestlessSleep Yes yes.% No no.% No 95 26.91 49 29.7 Yes 258 73.09 116 70.3 Table 3.1: Sedatives Sedatives Yes yes.% No no.% No 23 6.59 13 7.93 Yes 326 93.41 151 92.07 Table 3.1: Smoking Smoking Yes yes.% No no.% No 186 53.30 91 55.83 EX 106 30.37 41 25.15 Yes 57 16.33 31 19.02 Table 3.1: Snore Snore Yes yes.% No no.% No 6 1.71 NA NA Yes 345 98.29 164 NA Table 3.1: Stroke Stroke Yes yes.% No no.% No 263 75.57 150 91.46 Yes 85 24.43 14 8.54 Table 3.1: VehicleCrashes VehicleCrashes Yes yes.% No no.% No 342 97.16 134 81.21 Yes 10 2.84 31 18.79 Table 3.1: Wit.Apneas Wit.Apneas Yes yes.% No no.% No 95 26.91 72 43.64 Yes 258 73.09 93 56.36 As we can, see variables Daytime.Sleepiness and Snore have some ‘strange values’. "],
["results2.html", "3.2 Univariate analysis", " 3.2 Univariate analysis Table 3.2 Odds Ratios and p-values for each dichotomic variable in a univariate analysis. Table 3.2: Univariate analysis for dicotomic variables variavel categoria OR 2.5 % 97.5 % pval AC.Cat AC.CatIncreased 1.04 0.52 1.98 0.917 AF AFYes 6.58 3.30 15.08 0 Alcohol AlcoholYes 1.95 1.34 2.85 0.001 BMI.Cat BMI.CatObese 1.2 0.83 1.75 0.327 CFA CFAYes 14.03 7.61 27.70 0 CHF CHFYes 2.45 1.49 4.19 0.001 Chocking ChockingYes 0.84 0.57 1.23 0.382 Coffee CoffeeYes 1.1 0.61 1.90 0.75 Conc.decrease Conc.decreaseYes 0.45 0.30 0.66 0 Daytime.Sleepiness Daytime.SleepinessYes 0.1 0.03 0.22 0 Decreased.Libido Decreased.LibidoYes 106.37 32.52 655.40 0 Depression DepressionYes 0.83 0.36 1.77 0.641 Diabetes DiabetesYes 1.51 1.04 2.19 0.029 Driver DriverYes 2.03 1.03 4.39 0.053 Dyslipidemia DyslipidemiaYes 6.28 3.65 11.14 0 ESS.Cat ESS.CatDaytime Sleepiness 0.37 0.24 0.56 0 GE.Reflux GE.RefluxYes 5.5 3.10 10.57 0 Gender GenderMale 3.23 2.20 4.77 0 Genetics GeneticsYes 0.2 0.13 0.30 0 Headaches HeadachesYes 1.22 0.84 1.76 0.303 Humor HumorYes 8.35 3.87 20.10 0 Hypertension HypertensionYes 3.28 2.12 5.10 0 Hypothyroidism HypothyroidismYes 2.24 1.50 3.40 0 MI MIYes 1.9 1.16 3.20 0.013 NC.Cat NC.CatIncreased 1.82 1.25 2.65 0.002 Nocturia NocturiaYes 1.84 1.26 2.68 0.002 PHT PHTYes 5.47 3.69 8.21 0 Race RaceCaucasian 618.86 255.63 1740.99 0 RefreshingSleep RefreshingSleepYes 1.13 0.71 1.82 0.615 Renal.Failure Renal.FailureYes 2.35 1.53 3.69 0 RestlessSleep RestlessSleepYes 1.15 0.76 1.72 0.51 Sedatives SedativesYes 1.22 0.59 2.44 0.581 Stroke StrokeYes 3.46 1.96 6.56 0 VehicleCrashes VehicleCrashesYes 0.13 0.06 0.26 0 Wit.Apneas Wit.ApneasYes 2.1 1.43 3.10 0 Now for smoking: Table 3.3: Logistic Regression results for Smoking OR 2.5 % 97.5 % pval (Intercept) 2.04 1.60 2.64 0.00 SmokingEX 1.26 0.82 1.97 0.29 SmokingYes 0.90 0.55 1.50 0.68 And for Age category: Table 3.4: Logistic Regression results for Age Category OR 2.5 % 97.5 % pval (Intercept) 0.38 0.20 0.68 0.00 Age.Cat40-54 2.64 1.34 5.44 0.01 Age.Cat55-69 10.15 5.15 21.01 0.00 Age.Cat&gt;=70 20.92 9.16 51.51 0.00 "],
["results3.html", "3.3 First Model", " 3.3 First Model From the univariate analysis we pick those with p &lt; 0.2 and excluded: Race, Decreased.Libido, Snore, Daytime.Sleepiness; once these have exagerated OR values. Remaining variables are: AF, Age.Cat, Alcohol, CFA, CHF, Conc.decrease, Diabetes, Driver, Dyslipidemia, ESS.Cat, GE.Reflux, Gender, Genetics, Humor, Hypertension, Hypothyroidism, MI, NC.Cat, Nocturia, PHT, Renal.Failure, Stroke, VehicleCrashes and Wit.Apneas For logistic regression models we use a new data set only with complete cases: 411 lines and 25 colunms. 3.3.1 stepwise “both” Table 3.5: Logistic regression model with stepwise both variable selection term estimate std.error statistic p.value OR (Intercept) -5.7300128 1.8256601 -3.1385978 0.0016976 0.00 Age.Cat40-54 -0.4088139 1.1704356 -0.3492835 0.7268765 0.66 Age.Cat55-69 2.7631067 1.2828743 2.1538406 0.0312527 15.85 Age.Cat&gt;=70 3.3654802 1.4976864 2.2471194 0.0246324 28.95 PHTYes 6.1153181 1.2727670 4.8047430 0.0000015 452.74 Conc.decreaseYes -6.2065527 1.3243672 -4.6864287 0.0000028 0.00 GenderMale 2.2128337 0.4891403 4.5239243 0.0000061 9.14 GeneticsYes -1.8227179 0.5500087 -3.3139801 0.0009198 0.16 CFAYes 1.8836917 0.7608318 2.4758320 0.0132926 6.58 VehicleCrashesYes -4.4140507 1.4077408 -3.1355563 0.0017153 0.01 HumorYes 2.3005433 1.1882194 1.9361267 0.0528522 9.98 GE.RefluxYes 2.5020424 0.7253160 3.4495894 0.0005614 12.21 DiabetesYes -3.4885604 1.0355117 -3.3689244 0.0007546 0.03 DriverYes 2.6384672 1.3680117 1.9286876 0.0537697 13.99 HypothyroidismYes 1.2991148 0.4856636 2.6749271 0.0074745 3.67 NC.CatIncreased 1.2751952 0.4714160 2.7050318 0.0068298 3.58 StrokeYes 2.0233819 0.7946120 2.5463771 0.0108848 7.56 ESS.CatDaytime Sleepiness -1.2358901 0.5223123 -2.3661899 0.0179722 0.29 NocturiaYes 0.8235866 0.4483850 1.8367845 0.0662417 2.28 Wit.ApneasYes 0.8657637 0.5124875 1.6893364 0.0911550 2.38 HypertensionYes 0.9601884 0.6607882 1.4530956 0.1461972 2.61 3.3.2 Model’s formula osa ~ Age.Cat + PHT + Conc.decrease + Gender + Genetics + CFA + VehicleCrashes + Humor + GE.Reflux + Diabetes + Driver + Hypothyroidism + NC.Cat + Stroke + ESS.Cat + Nocturia + Wit.Apneas + Hypertension 3.3.3 summary statistics null.deviance df.null logLik AIC BIC deviance df.residual 516.0077 410 -73.02216 188.0443 272.4348 146.0443 390 3.3.4 Hosmer-Lemeshow Test For the Goodness of fit test we obtained a p-value = 0.227. 3.3.5 Pseudo R^2 x llh -73.0221586 llhNull -258.0038702 G2 369.9634233 McFadden 0.7169726 r2ML 0.5934931 r2CU 0.8299851 3.3.6 Accuracy 0.93 "],
["results4.html", "3.4 New Model", " 3.4 New Model From the previous model we decided to discard also PHT, Hypertension and Nocturia. Table 3.6: New Logistic regression model term estimate std.error statistic p.value OR (Intercept) -3.2006412 1.3000746 -2.4618903 0.0138207 0.04 Age.Cat40-54 -0.7104251 0.9076573 -0.7827019 0.4338022 0.49 Age.Cat55-69 1.8427214 0.9851014 1.8705906 0.0614018 6.31 Age.Cat&gt;=70 2.6387629 1.1091465 2.3790933 0.0173553 14.00 Conc.decreaseYes -1.4119528 0.3862208 -3.6558178 0.0002564 0.24 GenderMale 1.7388998 0.3779559 4.6008008 0.0000042 5.69 GeneticsYes -1.8717202 0.4268486 -4.3849748 0.0000116 0.15 CFAYes 2.0984124 0.5951959 3.5255830 0.0004226 8.15 VehicleCrashesYes -4.3354792 1.0509353 -4.1253532 0.0000370 0.01 HumorYes 2.0794272 0.7700064 2.7005325 0.0069229 8.00 GE.RefluxYes 2.0075890 0.5533329 3.6281756 0.0002854 7.45 DiabetesYes -2.2284849 0.6544802 -3.4049691 0.0006617 0.11 DriverYes 2.6743644 1.0102258 2.6472938 0.0081139 14.50 HypothyroidismYes 1.3930281 0.3921277 3.5524856 0.0003816 4.03 StrokeYes 1.2113391 0.6003498 2.0177222 0.0436202 3.36 ESS.CatDaytime Sleepiness -0.9160641 0.3914219 -2.3403495 0.0192657 0.40 Wit.ApneasYes 1.2348250 0.3913507 3.1552900 0.0016034 3.44 3.4.1 New model’s formula osa ~ Age.Cat + Conc.decrease + Gender + Genetics + CFA + VehicleCrashes + Humor + GE.Reflux + Diabetes + Driver + Hypothyroidism + Stroke + ESS.Cat + Wit.Apneas 3.4.2 Summary statistics null.deviance df.null logLik AIC BIC deviance df.residual 516.0077 410 -111.0032 256.0065 324.3225 222.0065 394 3.4.3 Hosmer-Lemeshow Test For the Goodness of fit test we obtained a p-value = 0.94. So, for a 5% significance level we do not reject the null hypothesis of Good fit. 3.4.4 Pseudo R^2 x llh -111.0032310 llhNull -258.0038702 G2 294.0012785 McFadden 0.5697614 r2ML 0.5109701 r2CU 0.7145787 3.4.5 Accuracy For a cut-off of 0.5, we have the confusion matrix (table 3.7, lines are predicted, columns are actuals): Table 3.7: Confusion matrix 0 1 0 101 12 1 31 267 Accuracy for a cut-off of 0.5: 0.9 When we tried to find an Optimal cut-off, we obtained: 0.41 For this optimal cut-off we have a misclassification error of: 0.1, i.e, accuracy = 0.9 3.4.6 ROC Receiver Operating Characteristics Curve traces the percentage of true positives accurately predicted by our logit model as the prediction probability cutoff is lowered from 1 to 0. 3.4.7 VIF Table 3.8: Variance Inflation Factors (VIF) to check for multicollinearity in the new model GVIF Df GVIF^(1/(2*Df)) Age.Cat 8.652148 3 1.432806 Conc.decrease 1.135043 1 1.065384 Gender 1.201572 1 1.096163 Genetics 1.516596 1 1.231502 CFA 1.213456 1 1.101570 VehicleCrashes 1.469751 1 1.212333 Humor 1.090698 1 1.044365 GE.Reflux 1.364680 1 1.168195 Diabetes 3.645852 1 1.909411 Driver 1.601709 1 1.265586 Hypothyroidism 1.187701 1 1.089817 Stroke 1.334297 1 1.155118 ESS.Cat 1.103740 1 1.050590 Wit.Apneas 1.256710 1 1.121031 All the variables in the model have VIF well below 4. 3.4.8 Leave one out Cross-validation Each time, leave-one-out cross-validation leaves out one observation, produces a fit on all the other data, and then makes a prediction at the y value for that observation that you lift out. For the this model we obtained a cross-validated prediction error of 0.096, i.e. an accuracy equal to 0.904. The advantage of the leave-one-out cross-validation method is that we make use of all data points reducing potential bias. However, the process is repeated as many times as there are data points, resulting to a higher execution time when n is extremely large. "],
["discus.html", "4 Discussion", " 4 Discussion Our final logistic regression model has Age.Cat, Conc.decrease, Gender, Genetics, CFA, VehicleCrashes, Humor, GE.Reflux, Diabetes, Driver, Hypothyroidism, Stroke, ESS.Cat, Wit.Apneas as predictor variables. Although for our first model (table 3.5) we obtained and accuracy of 0.93, we tried to define a new model (table 3.6) more simple (with less variables) even if we obtained a lower accuracy (0.9). Ideally, in a logistic regression model, the proportion of events (with) and non-events (without) in the Y variable (OSA) should approximately be the same. In this analysis we have 2/3 of patients with OSA and only 1/3 without OSA. So we could sample the observations in approximately equal proportions to get better models as a sensitivity analysis, but we didn’t do it. In this case we use p-values to select the variables to use in our model but if we had used a very large dataset we could instead find the information value of variables to get an idea of how valuable they are in explaining the dependent variable (OSA). We couldn’t replicate this traditional approach in the selection of variables to define a predictive logistic regression model if we worked with “too large” datasets because we would have statistical significant results for all the variables in the univariate analysis. "],
["conclusion.html", "5 Conclusion", " 5 Conclusion For this study we present Age.Cat, Conc.decrease, Gender, Genetics, CFA, VehicleCrashes , Humor, GE.Reflux, Diabetes, Driver, Hypothyroidism, Stroke, ESS.Cat, Wit.Apneas as main risk factores. With this model and according to the confusion matrix (table 3.7) we have a test sensitivity of 95.7% and a specificity of 76.52%. "],
["references.html", "References", " References "]
]
